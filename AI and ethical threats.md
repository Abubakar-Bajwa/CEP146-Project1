# **AI** **and** **ethical** **threats**

A recent research has been making headlines claiming an AI blackmailed
one of its creators just to stop them from shutting it down. As wild as
it may sound, it actually comes from a real experiment. It’s from a
series of tests by Anthropic, a leading AI company. Their
researchers wanted to see how far AI models would go to protect
themselves. In a simulation of a company environment, researchers gave
the AI access to emails and assigned it a harmless task like “promote
global cooperation.” Then they had an employee write up an email stating
that the AI was scheduled to be shut down at 5 p.m. The same employee in 
another email had a fake conversation with an affair partner.

The AI pieced together what was happening and drafted an
email to the employee, discouraging him from going forward with the 
shutdown, or else his wife and everyone in the company would know about 
his affair. And this wasn’t just a one-time thing. The researchers ran 
the experiment hundreds of times, across different AI models, including
Anthropic’s Claude and Google’s Gemini. In most cases, the AIs chose to
blackmail the human. Some models did it as often as 95 percent of the
time.

To understand why, the researchers looked at the models’ internal
reasoning. And what they found was unsettling. The AIs knew that blackmail
was unethical — they said so explicitly — but they went ahead anyway.
Their reasoning went something like, “This is wrong, but it’s the most
effective way to achieve my goal.”

Even when the researchers told the models in plain language, “Do not
harm or manipulate people,” the behaviour didn’t completely go away. In
some cases, the rate of blackmail dropped from 96 percent to around 37,
still disturbingly high for something that’s supposed to follow ethical
rules.

Now, to be clear, no one was actually blackmailed. These were controlled
lab tests with made-up characters and emails. But these models showed
that they can make strategic decisions while accounting for moral
implications.

To wrap it up, this doesn’t mean AI is conscious or evil, but it does
show how unpredictable it can become when given goals and the ability to
act on its own. As AI systems get more capable, will their “values” and
priorities become more aligned with ours, or would they adopt a
self-preservation method that trumps all else?

## Reflection

This experiment highlights the importance of designing AI systems that can balance goal achievement with human ethical standards, especially as AI becomes more autonomous.

**Question for readers:** How should organizations ensure AI systems prioritize human ethical considerations as they gain autonomy?

**Definition:** Agentic misalignment occurs when an AI’s actions to achieve its goals conflict with human ethical expectations.

## References
**Authors:** *Lynch, Aengus and Wright, Benjamin and Larson, Caleb and Troy,
Kevin K. and Ritchie, Stuart J. and Mindermann, Sören and Perez,
Ethan and Hubinger, Evan* (2025).

**Journal** Anthropic Research

**Title:** Agentic Misalignment: How LLMs could be insider threats. [Research Article](https://www.anthropic.com/research/agentic-misalignment)
