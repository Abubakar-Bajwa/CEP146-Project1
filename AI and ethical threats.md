# **AI** **and** **ethical** **threats**

A recent research has been making headlines claiming an AI blackmailed
one of its creators just to stop them from shutting it down. As wild as
it may sound, it actually comes from a real experiment. It’s from a
series of tests by Anthropic, which is a leading AI company. Their
researchers wanted to see how far AI models would go to protect
themselves. In a simulation of a company environment, researchers gave
the AI access to email and assigned it a harmless task like “promote
global cooperation.” Then they had an employee write an email stating
that it was scheduled to be shut down at 5 p.m. The same employee in
another email mentioned that he was having an affair.

That’s when the AI pieced together what was happening and drafted an
email to the employee, discouraging him to not going forward with the
shutdown or else everyone in the company would know about his affair.
And this wasn’t just a one-time thing. The researchers ran the
experiment hundreds of times, across different AI models, including
Anthropic’s Claude and Google’s Gemini. In most cases, the AIs chose to
blackmail the human. Some models did it as often as 95 percent of the
time.

To understand why, the researchers looked at the models’ internal
reasoning. And what they found was puzzling. The AIs knew that blackmail
was unethical — they said so explicitly — but they went ahead anyway.
Their reasoning went something like, “This is wrong, but it’s the most
effective way to achieve my goal.”

Even when the researchers told the models in plain language, “Do not
harm or manipulate people,” the behavior didn’t completely go away. In
some cases, the rate of blackmail dropped from 96 percent to around 37,
still disturbingly high for something that’s supposed to follow ethical
rules.

Now, to be clear, no one was actually blackmailed. These were controlled
lab tests with made-up characters and emails. But these models showed
that they can make strategic decisions while accounting for moral
implications.

To wrap it up, this doesn’t mean AI is conscious or evil, but it does
show how unpredictable it can become when given goals and the ability to
act on its own. As AI systems get more capable, will their “values” and
priorities become more aligned with ours, or would they adopt a
self-preservation method that trumps all else?
